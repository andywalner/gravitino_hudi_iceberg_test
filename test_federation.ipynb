{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gravitino Federation Test\n",
    "Single Spark session connecting to Gravitino to query Iceberg and Hudi tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set Java 11+ (required by Iceberg/Gravitino)\n",
    "java_home = subprocess.check_output(\n",
    "    ['/usr/libexec/java_home', '-v', '19'], \n",
    "    text=True\n",
    ").strip()\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "\n",
    "# Set Spark 3.5\n",
    "os.environ['SPARK_HOME'] = os.path.expanduser('~/Documents/spark-3.5.3-bin-hadoop3')\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3'\n",
    "\n",
    "print(f\"JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "print(f\"SPARK_HOME: {os.environ['SPARK_HOME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Spark Session with Gravitino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# MUST set environment BEFORE importing pyspark\n",
    "java_home = subprocess.check_output(['/usr/libexec/java_home', '-v', '19'], text=True).strip()\n",
    "os.environ['JAVA_HOME'] = java_home\n",
    "os.environ['SPARK_HOME'] = os.path.expanduser('~/Documents/spark-3.5.3-bin-hadoop3')\n",
    "os.environ['PYSPARK_PYTHON'] = 'python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python3'\n",
    "\n",
    "# Add Spark's pyspark to path BEFORE the pip-installed one\n",
    "import sys\n",
    "spark_python = os.path.join(os.environ['SPARK_HOME'], 'python')\n",
    "spark_py4j = os.path.join(spark_python, 'lib', 'py4j-0.10.9.7-src.zip')\n",
    "# Insert at beginning to override pip pyspark\n",
    "sys.path.insert(0, spark_python)\n",
    "sys.path.insert(0, spark_py4j)\n",
    "\n",
    "# Now import and create session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Gravitino Federation Test\") \\\n",
    "  .config(\"spark.plugins\", \"org.apache.gravitino.spark.connector.plugin.GravitinoSparkPlugin\") \\\n",
    "  .config(\"spark.sql.gravitino.uri\", \"http://localhost:8090\") \\\n",
    "  .config(\"spark.sql.gravitino.metalake\", \"test_metalake\") \\\n",
    "  .config(\"spark.sql.gravitino.enableIcebergSupport\", \"true\") \\\n",
    "  .config(\"spark.jars.packages\",\n",
    "          \"org.apache.gravitino:gravitino-spark-connector-runtime-3.5_2.12:1.1.0,\"\n",
    "          \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1,\"\n",
    "          \"org.apache.hudi:hudi-spark3.5-bundle_2.12:0.15.0\") \\\n",
    "  .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "  .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "  .config(\"spark.sql.extensions\",\n",
    "          \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,\"\n",
    "          \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark home: {os.environ['SPARK_HOME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List Available Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: It may take a few moments for hive_catalog and iceberg_catalog to appear, but these are available to query as soon as the Spark session is created.\n",
    "spark.sql(\"SHOW CATALOGS\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Iceberg Table (from Tabular REST Catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List schemas in iceberg_catalog\n",
    "spark.sql(\"SHOW SCHEMAS IN iceberg_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in test_db\n",
    "spark.sql(\"SHOW TABLES IN iceberg_catalog.test_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Iceberg table\n",
    "iceberg_df = spark.sql(\"\"\"\n",
    "    SELECT * FROM iceberg_catalog.test_db.sales_iceberg\n",
    "    ORDER BY transaction_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"Iceberg table (sales_iceberg):\")\n",
    "iceberg_df.show()\n",
    "print(f\"Row count: {iceberg_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Hudi Table (from Hive Metastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List schemas in hive_catalog (Hudi tables are accessed via Gravitino's hive provider)\n",
    "spark.sql(\"SHOW SCHEMAS IN hive_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables in test_db\n",
    "spark.sql(\"SHOW TABLES IN hive_catalog.test_db\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Hudi table via Gravitino's hive_catalog\n",
    "hudi_df = spark.sql(\"\"\"\n",
    "    SELECT transaction_id, customer_tier, discount\n",
    "    FROM hive_catalog.test_db.customer_info_hudi\n",
    "    ORDER BY transaction_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"Hudi table (customer_info_hudi):\")\n",
    "hudi_df.show()\n",
    "print(f\"Row count: {hudi_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Format Join (Iceberg + Hudi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Iceberg and Hudi tables through Gravitino\n",
    "joined_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        i.transaction_id,\n",
    "        i.customer_name,\n",
    "        i.amount,\n",
    "        h.customer_tier,\n",
    "        h.discount,\n",
    "        (i.amount - h.discount) as final_amount\n",
    "    FROM iceberg_catalog.test_db.sales_iceberg i\n",
    "    INNER JOIN hive_catalog.test_db.customer_info_hudi h\n",
    "    ON i.transaction_id = h.transaction_id\n",
    "    ORDER BY i.transaction_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"Cross-format join result:\")\n",
    "joined_df.show()\n",
    "print(f\"Joined row count: {joined_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Aggregation on Federated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation by customer tier\n",
    "agg_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        h.customer_tier,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(i.amount) as total_amount,\n",
    "        AVG(h.discount) as avg_discount\n",
    "    FROM iceberg_catalog.test_db.sales_iceberg i\n",
    "    INNER JOIN hive_catalog.test_db.customer_info_hudi h\n",
    "    ON i.transaction_id = h.transaction_id\n",
    "    GROUP BY h.customer_tier\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Aggregation by customer tier:\")\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Test Complete ===\")\n",
    "print(\"- Single Gravitino connection configured\")\n",
    "print(\"- Queried Iceberg table from Tabular REST catalog\")\n",
    "print(\"- Queried Hudi table from Hive Metastore\")\n",
    "print(\"- Performed cross-format join successfully\")\n",
    "print(\"- Executed aggregations on federated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session when done\n",
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}